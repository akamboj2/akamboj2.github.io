<!DOCTYPE html> 
<html>
    <head>
        <link rel="stylesheet" href="styles.css">
        <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
    </head>
    <div class="container">
    <script>
        $.get("navigation.html", function(data){
            $("#nav-placeholder").replaceWith(data);
        });
    </script>
    <div id="nav-placeholder">
    </div>
    <div class="main_text">
        <h1 id="projects"> Projects </h1>


        <!-- Sensor Fusion Project -->
        <!-- <button class="collapsible">Multi Sensor Multi Task Fusion</button>
        <div class="collapse_content"> 
            <p>
                <b>Description:</b> 
                This project aims to develop a model capable of human behavior and motion understanding a prediction.
                Details are outlined in the following proposal: <a href="files/Abhi_Kamboj_Research_Plan_Submit.pdf">Mulit Sensor Multi Task Fusion</a>
            </p>
        </div> -->

        <!--  GOOD TEMPLATE -->
        <!-- Tag Project -->
        <!-- <button class="collapsible">
            <img src="files/todo-qm.jpg" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">TITLE</div>
                <div class="project-authors">Abhi Kamboj</div>
                <div class="project-venue">Arxiv 2024</div>
                <div class="project-links">
                    <a href="link-to-paper">Paper</a> | 
                    <a href="link-to-code">Code</a> | 
                    <a href="link-to-website">Website</a>
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br> 

                 
                <figure>
                    <img src="files/todo-qm.jpg" alt="Global Tracking Transformer Architecture" style="width:30%">
                    <figcaption>Sample caption goes here</figcaption>
                </figure>
            </p>
        </div> -->
    
        <!-- Robult -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2025/robult/pid.png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title"> Robult: Leveraging Redundancy and Modality-Specific Features for Robust Multimodal Learning</div>
                <div class="project-authors">Duy A. Nguyen, Abhi Kamboj, Minh N. Do</div>
                <div class="project-venue">IJCAI 2025</div>
                <div class="project-links">
                    <a href="https://ijcai-preprints.s3.us-west-1.amazonaws.com/2025/7733.pdf">Paper</a>  <!-- |--> 
                    <!-- <a href="link-to-code">Code</a> |  -->
                    <!-- <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br> 
                    Addressing missing modalities and limited labeled data is crucial for advancing robust multimodal learning. We propose Robult, a scalable framework designed to mitigate these challenges by preserving modality-specific information and leveraging redundancy through a novel information-theoretic approach. Robult optimizes two core objectives: (1) a soft Positive-Unlabeled (PU) contrastive loss that maximizes task-relevant feature alignment while effectively utilizing limited labeled data in semi-supervised settings, and (2) a latent reconstruction loss that ensures unique modality-specific information is retained. These strategies, embedded within a modular design, enhance performance across various downstream tasks and ensure resilience to incomplete modalities during inference. Experimental results across diverse datasets validate that Robult achieves superior performance over existing approaches in both semi-supervised learning and missing modality contexts. Furthermore, its lightweight design promotes scalability and seamless integration with existing architectures, making it suitable for real-world multimodal applications.
                <figure>
                    <img src="Abhi Research Portfolio/2025/robult/robult_architecture.png" alt="Global Tracking Transformer Architecture" style="width:80%">
                    <figcaption>Overall Architecture and Flow of the Robult Method</figcaption>
                </figure>
            </p>
        </div>
    
        <!-- Perfect Alignment -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2025/perfect_alignment/PA_teaser_half.png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">Towards Achieving Perfect Multimodal Alignment</div>
                <div class="project-authors">Abhi Kamboj, Minh N. Do</div>
                <div class="project-venue">Arxiv 2025</div>
                <div class="project-links">
                    <a href="https://arxiv.org/abs/2503.15352">Paper</a> 
                    <!-- <a href="link-to-code">Code</a> | 
                    <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br> 
                Multimodal alignment constructs a joint latent vector space where modalities representing the same concept map to neighboring latent vectors. We formulate this as an inverse problem and show that, under certain conditions, paired data from each modality can map to equivalent latent vectors, which we refer to as perfect alignment. When perfect alignment cannot be achieved, it can be approximated using the Singular Value Decomposition (SVD) of a multimodal data matrix. Experiments on synthetic multimodal Gaussian data verify the effectiveness of our perfect alignment method compared to a learned contrastive alignment method. We further demonstrate the practical application of cross-modal transfer for human action recognition, showing that perfect alignment significantly enhances the model's accuracy. We conclude by discussing how these findings can be applied to various modalities and tasks and the limitations of our method. We hope these findings inspire further exploration of perfect alignment and its applications in representation learning.
                <figure>
                    <img src="Abhi Research Portfolio/2025/perfect_alignment/PA_teaser.png" alt="Perfect Alignment Diagram" style="width:80%">
                    <figcaption>Hypothesized Perfect Alignment Framework</figcaption>
                </figure>
            </p>
        </div>

        <!-- C3T fusion and transfer -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2024/FACT/T-Fact_img.png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">Cross Modal Transfer Through Time (C3T) for Unsuperivised Modality Adaptation in Human Action Recognition</div>
                <div class="project-authors">Abhi Kamboj, Duy Anh Nguyen, Minh Do</div>
                <div class="project-venue">Arxiv 2024, Under review at Conference</div>
                <div class="project-links">
                    <!-- <a href="Abhi Research Portfolio/2024/ARXIV_C3T_Crossmodal_Transfer_Through_Time.pdf">Paper</a>   -->
                    <a href="https://arxiv.org/abs/2407.16803v3">Paper</a>  
                    <!-- <a href="link-to-code">Code</a>| 
                    <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br>
                    In order to unlock the potential of diverse sensors, we investigate a method to transfer knowledge between modalities using the structure of a unified multimodal representation space for Human Action Recognition (HAR). We formalize and explore an understudied cross-modal transfer setting we term Unsupervised Modality Adaptation (UMA), where the modality used in testing is not used in supervised training, i.e. zero labeled instances of the test modality are available during training. We develop three methods to perform UMA: Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal Transfer Through Time (C3T). Our extensive experiments on various camera+IMU datasets compare these methods to each other in the UMA setting, and to their empirical upper bound in the supervised setting. The results indicate C3T is the most robust and highest performing by at least a margin of 8%, and nears the supervised setting performance even in the presence of temporal noise. This method introduces a novel mechanism for aligning signals across time-varying latent vectors, extracted from the receptive field of temporal convolutions. Our findings suggest that C3T has significant potential for developing generalizable models for time-series sensor data, opening new avenues for multi-modal learning in various applications.
                <figure>
                    <img src="Abhi Research Portfolio/2024/FACT/T-Fact_img.png" alt="Time continuous extension of FACT (T-FACT) Diagram" style="width:80%">
                    <figcaption>A diagram of the C3T</figcaption>
                </figure>
            </p>
        </div>
        
        <!-- FACT fusion and transfer -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2024/FACT/FACT overview.png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">FACT: Fusion and Cross-Modal Transfer for Zero-Shot Human Action Recognition </div>
                <div class="project-authors">Abhi Kamboj, Minh Do</div>
                <div class="project-venue">7th Illinois and Health Summit - Healthy Aging of Brain and Mind with AI - Best Poster Award 3<sup>rd</sup> place</div>
                <div class="project-links">
                    <!-- <a href="link-to-paper">Paper</a>| 
                    <!-- <a href="link-to-code">Code</a>|  -->
                    <a href="files/projects/health_ai_summit_2024.pdf">Poster</a> 
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br>
                Despite living in a multi-sensory world, most AI models are limited to textual and visual interpretations of human motion and behavior. 
                Inertial measurement units (IMUs) provide a salient signal to understand human motion; however, they are challenging to use due to their uninterpretability and scarcity of their data. 
                We investigate a method to transfer knowledge between visual and inertial modalities using the structure of an informative joint representation space designed for human action recognition (HAR). 
                We apply the resulting Fusion and Cross-modal Transfer (FACT) method to a novel setup, where the model does not have access to labeled IMU data during training and is able to perform HAR with only IMU data during testing.
                Extensive experiments on a wide range of RGB-IMU datasets demonstrate that FACT significantly outperforms existing methods in zero-shot cross-modal transfer.
                
                <figure>
                    <img src="Abhi Research Portfolio/2024/FACT/FACT overview.png" alt="FACT architecture overview" style="width:40%">
                    <img src="Abhi Research Portfolio/2024/FACT/T-Fact_img.png" alt="Time continuous extension of FACT (T-FACT) Diagram" style="width:40%">
                    <figcaption>Left: An overview of the FACT method. Right: A diagram of the time continous FACT model T-FACT.</figcaption>
                </figure>
            </p>
        </div>



        <!-- Cross-modal transfer lit review -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2024/cross-modal_review/HAR_metrics (1).png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">A Survey of IMU Based Cross-Modal Transfer Learning in Human Activity Recognition
                </div>
                <div class="project-authors">Abhi Kamboj, Minh Do</div>
                <div class="project-venue">M.S. Thesis 2024</div>
                <div class="project-links">
                    <a href="https://arxiv.org/abs/2403.15444">Paper</a> 
                    <!--  <a href="link-to-code">Code</a>| 
                    <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br>
                Despite living in a multi-sensory world, most AI models are limited to textual and visual understanding of human motion and behavior. 
                Inertial measurement sensors provide a signal for AI to understand motion, however, in practice they has been understudied due to numerous difficulties and the uniterpretability of the data to humans. 
                In fact, full situational awareness of human motion could best be understood through a combination of sensors.
                In this survey we investigate how knowledge can be transferred and utilized amongst modalities for Human Activity/Action Recognition (HAR), i.e. cross-modality transfer learning.
                We motivate the importance and potential of IMU data and its applicability in cross-modality learning as well as the importance of studying the HAR problem. 
                We categorize HAR related tasks by time and abstractness and then compare various types of multimodal HAR datasets.
                We also distinguish and expound on many related but inconsistently used terms in the literature, such as transfer learning, domain adaptation, representation learning, sensor fusion, and multimodal learning, and describe how cross-modal learning fits with all these concepts.
                We then review the literature in IMU-based cross-modal transfer for HAR.
                The two main approaches for cross-modal transfer are instance-based transfer, where instances of one modality are mapped to another (e.g. knowledge is transferred in the input space), or feature-based transfer, where the model relates the modalities in an intermediate latent space (e.g. knowledge is transferred in the feature space).
                Finally, we discuss future research directions and applications in cross-modal HAR.

                <div class="figure-container">
                    <figure>
                        <img src="Abhi Research Portfolio/2024/cross-modal_review/HAR_metrics (1).png" alt="Global Tracking Transformer Architecture">
                        <figcaption> A graph-like visualization plotting HAR-related tasks to compare each task with the other based on temporal length involved in the task as well as the abstractness or coarseness of the task.</figcaption>
                    </figure>
                    <figure>
                        <img src="Abhi Research Portfolio/2024/cross-modal_review/venn_diagram.png" alt="Global Tracking Transformer Architecture">
                        <figcaption>A comparison of sensor fusion and cross modal transfer.</figcaption>
                    </figure>
                    <figure>
                        <img src="Abhi Research Portfolio/2024/cross-modal_review/cross_modal_learning (1).png" alt="Global Tracking Transformer Architecture">
                        <figcaption>A diagram showing two types of cross-modal transfer.</figcaption>
                    </figure>

                </div>
            </p>
        </div>


        
        <!-- Netradyne DriverGPT Project -->
        <button class="collapsible">
            <img src="files/netradyne/netradyne_dash_cam.jpeg" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">DriverGPT: Training a Generative Pretrained Transformer on Time-continous Physical Data</div>
                <div class="project-authors">Abhi Kamboj, Ananya Gupta, Michael Laelli</div>
                <div class="project-venue">Netradyne Artificial Intellgience Internship, San Diego, CA, 2022</div>
                <div class="project-links">
                    <!-- <a href="link-to-paper">Paper</a>| 
                    <a href="link-to-code">Code</a>| 
                    <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Description:</b> <br>
                Generative Pretrained Transformers (GPT), e.g., ChatGPT, have recently changed the landscape of AI research with their size, scale, and performance. Researchers have discovered that with ample data and compute resources, this architecture performs very well and shows "emergent behavior," i.e., the ability to perform well or demonstrate knowledge on tasks it was not explicitly trained on. Netradyne is a fleet safety company that focuses on dash camera driving analytics, and they have vast amounts of driver data, providing a unique setting to leverage GPT-style models. The DriverGPT project aims to learn to predict future driver behaviors based on previous time steps. This could be used to predict short-term trajectories, such as potential collisions, or long-term behaviors, such as the overall trends of a driver's safety score and how it compares to other drivers. Nonetheless, extending GPT to driving data presented many challenges, such as how to tokenize continuous data or how to use GPT on time series data from devices with different frame rates and specifications. Many small-scale experiments were conducted to address these questions.
            <!-- 
                <figure>
                    <img src="files/netradyne/GTR.png" alt="Global Tracking Transformer Architecture" style="width:30%">
                    <figcaption>Transformer architectures for multi object tracking (ours is called Trackformer++)</figcaption>
                </figure> -->
            </p>
        </div>

        <!-- Robotics Pretraining Project -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2023/robot_graping_figs/affordance-img-thumbnail.png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">A Brief Survey on Leveraging Large Scale Vision Models for Enhanced Robot Grasping</div>
                <div class="project-authors">Abhi Kamboj, Katherine Driggs-Campbell</div>
                <!-- <div class="project-description">The goal of this internship project was to create a novel multi object tracking architecture using transformers.</div> -->
                <div class="project-venue">Arxiv 2023</div>
                <div class="project-links">
                    <a href="https://arxiv.org/abs/2406.11786">Paper</a>
                    <!-- <a href="link-to-code">Code</a>| 
                    <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br>
                Robotic grasping presents a difficult motor task in real-world scenarios, constituting a major hurdle to the deployment of capable robots across various industries. 
                Notably, the scarcity of data makes grasping particularly challenging for learned models. 
                Recent advancements in computer vision have witnessed a growth of successful unsupervised training mechanisms predicated on massive amounts of data sourced from the Internet, and now nearly all prominent models leverage pretrained backbone networks. 
                Against this backdrop, we begin to investigate the potential benefits of large-scale visual pretraining in enhancing robot grasping performance. 
                This preliminary literature review sheds light on critical challenges and delineates prospective directions for future research in visual pretraining for robotic manipulation.
                
                <figure>
                    <img src="Abhi Research Portfolio/2023/robot_graping_figs/affordance-img.png" alt="Affordance prediction used for predicting robot grasps" style="width:30%">
                    <figcaption>This illustrates how visual pretraining can be used to teach affordance prediction models that guide a robot manipulator to the best picking position. This image is from <a /href="https://arxiv.org/pdf/2107.00646">"Learning to See before Learning to Act: Visual Pre-training for Manipulation
""                    </a></figcaption>
                </figure>
            </p>
        </div>
        

        <!-- MOT Transformers Survey -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2023/transformers_review_figs/TransTrack.png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">The Progression of Transformers from Language to Vision to MOT: A Literature Review on Multi-Object Tracking with Transformers</div>
                <div class="project-authors">Abhi Kamboj</div>
                <div class="project-venue">Arxiv 2023</div>
                <div class="project-links">
                    <a href="https://arxiv.org/abs/2406.16784">Paper</a> 
                    <!-- <a href="link-to-code">Code</a>| 
                    <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br>
                The transformer neural network architecture allows for autoregressive sequence-to-sequence modeling through the use of attention layers. It was originally created with the application of machine translation but has revolutionized natural language processing. Recently, transformers have also been applied across a wide variety of pattern recognition tasks, particularly in computer vision. In this literature review, we describe major advances in computer vision utilizing transformers. We then focus specifically on Multi-Object Tracking (MOT) and discuss how transformers are increasingly becoming competitive in state-of-the-art MOT works, yet still lag behind traditional deep learning methods.
                
                <figure>
                    <img src="Abhi Research Portfolio/2023/transformers_review_figs/Transformer.png" alt="Global Tracking Transformer Architecture" style="width:30%">
                    <img src="Abhi Research Portfolio/2023/transformers_review_figs/TransTrack.png" alt="Global Tracking Transformer Architecture" style="width:30%">
                    <figcaption>The left shows the original <a href="https://arxiv.org/abs/1706.03762"> Transformer </a> architectecture. The right shows an extension to the architurecture of object tracking, implemented as <a href="https://arxiv.org/abs/2012.15460">TransTrack</a> </figcaption>
                </figure> 
            </p>
        </div>



        <!-- Netradyne MOT Project -->
        <button class="collapsible">
            <img src="files/netradyne/Trackformer++.png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">Multi Object Tracking with Transformers</div>
                <div class="project-authors">Abhi Kamboj, Michael Laelli</div>
                <!-- <div class="project-description">The goal of this internship project was to create a novel multi object tracking architecture using transformers.</div> -->
                <div class="project-venue">Netradyne Machine Learning Internship, San Diego, CA, 2022</div>
                <div class="project-links">
                    <!-- <a href="link-to-paper">Paper</a>| 
                    <a href="link-to-code">Code</a>| 
                    <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Description:</b> <br>
                The goal of this internship project was to create a novel multi object tracking architecture using transformers. 
                I investigated existing Multi Object trakcing methods such as Global Tracking Transformers (GTR), Trackformer and MOTR and sought to improve them.
                All the existing methods do not utilize a transformers capability to deal with long range dependencies.
                GTR feeds in detections to a transformer to perform tracking over a window of frames, however, they lose contextual information by only feeding in detections, and they rely heavily on an accurate detector.
                On the other hand, Trackformer and MOTR autogressively feed in pairs of frames, not just detections, to detect and track objects in the next frame, however, they only work with a window of 2 frames at once.
                During this internship I helped develop an object tracking architecture that fuses multiple frames of features into the transformer and outputs multiple frames of object detections and tracks as output.
                
                <br><br>
                Work on this project is ongoing and experiments are being conducted. We refer to our model as <b>Trackformer++</b> as it builds off of Trackformer. A visual comparison is shown below.
                <figure>
                    <img src="files/netradyne/GTR.png" alt="Global Tracking Transformer Architecture" style="width:30%">
                    <img src="files/netradyne/Trackformer.png" alt="Trackformer Architecture" style="width:30%">
                    <img src="files/netradyne/Trackformer++.png" alt="Trackformer++ (Our) Architecture" style="width:25%">
                    <figcaption>Transformer architectures for multi object tracking (ours is called Trackformer++)</figcaption>
                </figure>
            </p>
        </div>


        <!-- Agbot Project -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2021 - Senior Year/auto_farm30x30.gif" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">Examining Audio Communication Mechanisms for Supervising Fleets of Agricultural Robots</div>
                <div class="project-authors">Abhi Kamboj, Tianchen Ji, Katherine Driggs-Campbell</div>
                <!-- <div class="project-description">This is a research project investigating various forms of speech/audio communication for managing a fleet of autonomous robots.</div> -->
                <div class="project-venue">Oral in 2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)
                </div>
                <div class="project-links">
                    <a href="https://ieeexplore.ieee.org/document/9900859">Paper</a> | 
                    <a href="https://github.com/akamboj2/Agbot-Sim">Code</a> 
                    <!-- <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <figure>
                    <img src="Abhi Research Portfolio/2021 - Senior Year/auto_farm30x30.gif" alt="Semantic segmentation of a building" style="width:20%">
                    <img src="Abhi Research Portfolio/2021 - Senior Year/small_grid_errors.png" alt="Semantic segmentation of a building" style="width:30%">
                    <img src="Abhi Research Portfolio/2021 - Senior Year/terrasentia.jpg" alt="Semantic segmentation of a building" style="width:30%">
                    <figcaption>Example of the simulation running, user interface, and real robots they represent.</figcaption>
                </figure>

                <b>Abstract:</b>
                <br>
                Agriculture is facing a labor crisis, leading to increased interest in fleets of small, under-canopy robots (agbots) that can perform precise, targeted actions (e.g., crop scouting, weeding, fertilization), while being supervised by human operators remotely. 
                However, farmers are not necessarily experts in robotics technology and will not adopt technologies that add to their workload or do not provide an immediate payoff.
                In this work, we explore methods for communication between a remote human operator and multiple agbots and examine the impact of audio communication on the operator's preferences and productivity.
                We develop a simulation platform where agbots are deployed across a field, randomly encounter failures, and call for help from the operator. 
                As the agbots report errors, various audio communication mechanisms are tested to convey which robot failed and what type of failure occurs. 
                The human is tasked with verbally diagnosing the failure while completing a secondary task. 
                A user study was conducted to test three audio communication methods: earcons, single-phrase commands, and full sentence communication. 
                Each user completed a survey to determine each method's overall effectiveness and preferences. 
                Our results suggest that the system using short phrases is the most positively perceived by participants and may allow for the human to complete the secondary task more efficiently. 
                <br><br>
                <b>Full paper accepted as Oral Presentation @ IEEE ROMAN 2022, Naples Italy: <a href="Abhi Research Portfolio/2022 - Grad School/ROMAN_2022_Audio_Interaction (1).pdf">Examining Audio Communication Mechanisms for Supervising Fleets of Agricultural Robots</a></b>
            </p>
        </div>
        
        <!--  ECE 527 Project FPGA -->
        <button class="collapsible">
            <img src="files/projects/course_projects/ece527/basic_block_scalecuda.png" alt="ScaleCuda Block Diagram" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">ScaleCUDA: A New GPU Programming Framework on Multi-Level Intermediate Representation through ScaleHLS</div>
                <div class="project-authors">Abhi Kamboj, Armandeep Singh, Hanchen Ye, Deming Chen</div>
                <div class="project-venue">System on Chip Design Course Project 2022</div>
                <div class="project-links">
                    <a href="files/projects/course_projects/ece527/ECE_527_Final_Project_Final_Report.pdf">Paper</a> | 
                    <a href="https://github.com/akamboj2/scalehls/tree/scalecuda">Implementation-Code</a> | 
                    <a href="https://github.com/akamboj2/CuBLAS_ScaleCUDATesting">Test-Code</a>
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br> 
                Graphics Processing Units (GPUs) and manycore processors in general are some of the most important and powerful tools in modern computing as their ability to massively parallelize computations is an excellent way to accelerate computationally intensive programs.  Due to the massive presence of GPUs in heterogeneous systems, the large design space of parallel programming, and the substantial variance in performance between low and high end implementations of GPU kernels, effectively developing this software is a critical challenge. The development and optimization process can be automated. Successful projects in this area take advantage of the affine properties of the input functionality as identified by polyhedral models in order to output optimized kernels for manycore processors.

                This paper proposes ScaleCUDA, a tool for automated GPU code (CUDA) generation and optimization. ScaleCUDA looks to take advantage of two essential components of high end optimization: identification of affine properties through polyhedral modelling and analysis at multiple levels of abstraction. It will do so by way of ScaleHLS\cite{ye2022scalehls}, a high-level synthesis (HLS) design space exploration tool. ScaleHLS uses MLIR, a multi-level compiler infrastructure, in order to effectively explore the design space at various levels of abstraction and create an optimized HLS design. To translate C++ or HLS C++ input into the desired optimal CUDA output, ScaleCUDA bridges the gap between the polyhedral and CUDA representations in MLIR while taking advantage of the HLS optimization and design space exploration tools of ScaleHLS. We test the pipeline with GEMM C++ code and show similar performance to code directly compiled and optimized through NVIDIA cuda frameworks. 
                
                Our implementation is open-sourced and can be accessed on the ScaleCUDA branch here: <a href="https://github.com/akamboj2/scalehls/tree/scalecuda">ScaleCUDA branch</a>. Our experimentation code is also provided here: <a href="https://github.com/akamboj2/CuBLAS_ScaleCUDATesting">Experimentation Code</a>.
                <div style="display: flex; justify-content: space-between; gap: 10px;">
                    <figure style="flex: 1; text-align: center;">
                        <img src="files/projects/course_projects/ece527/basic_block_scalecuda.png" alt="Basic Block Diagram of ScaleCUDA within MLIR" style="width:100%;">
                        <figcaption>The black lines and boxes represent preexisting components in the MLIR framework. The blue arrows represent ScaleCUDA proposed contributions. Polygiest translates from C++ to the Affine dialect. ScaleHLS translates from HLS C++ to the HLS dialect and the Affine dialect. ScaleCUDA's current implementation lowers the Affine dialect to the NVVM GPU Dialect and future extensions may convert from HLS to NVVM. </figcaption>
                    </figure>
                    <figure style="flex: 1; text-align: center;">
                        <img src="files/projects/course_projects/ece527/ScaleCUDA_pipeline.png" alt="block diagram" style="width:100%;">
                        <figcaption>This block diagram shows the entire ScaleCUDA pipeline. ScaleCUDA lowers the Affine dialect to the GPU dialect leveraging existing ScaleHLS passes and custom passes.</figcaption>
                    </figure>
                    <figure style="flex: 1; text-align: center;">
                        <img src="files/projects/course_projects/ece527/MatrixAdd_data.png" alt="ScaleCUDA Matrix Addition Performance" style="width:100%;">
                        <figcaption>The box plot above shows a comparison of performance of ScaleCUDA and CuBLAS. ScaleCUDA performs marginally better, however, with a higher variance.</figcaption>
                    </figure>
                    <figure style="flex: 1; text-align: center;">
                        <img src="files/projects/course_projects/ece527/MatrixMult_data.png" alt="ScaleCUDA Matrix Multiplication Performance" style="width:100%;">
                        <figcaption>ScaleCUDA Matrix Multiplication Performance: The box plot above shows a comparison of performance of ScaleCUDA, CuBLAS, and the CUDA MMA Op across 5 matrix multiplication trials for each method. ScaleCUDA shows a slightly higher performance in this simple metric.</figcaption>
                    </figure>
                </div>
            </p>
        </div>
    

        <!-- NVIDIA STR PROJECT -->
        <button class="collapsible">
            <img src="files/nvidia/STR_exampl2_airport.PNG" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">Developing Scene Text Recognition on Jetson Devices using TensorRT</div>
                <div class="project-authors">Abhi Kamboj</div>
                <!-- <div class="project-description">Using TensorRT to develop scene text recogntion models on the Jetson AGX Xavier and benchmarking SOTA models. </div> -->
                <div class="project-venue">NVIDIA Embedded AI Internship, San Jose, CA, 2021</div>
                <div class="project-links">
                    <!-- <a href="link-to-paper">Paper</a>|  -->
                    <a href="https://github.com/NVIDIA-AI-IOT/scene-text-recognition">Code</a>
                    <!-- <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Description:</b><br>
                I worked in NVIDIA’s Autonomous machines unit, under the Jetson Dev Tech team. 
                The Jetson product line is NVIDIA’s embedded AI GPU for edge IoT and mobile robotics applications. 
                The goal of my project was to develop an open-source Scene Text Recognition (STR) system for NVIDIA partners to use, as well as for NVIDIA’s own internal projects to use. 
                I initially researched and benchmarked various state of the art STR models (CSTR, STRN, EasyOCR) and chose the 2 stage EasyOCR framework to further develop, as it performed the best. 
                The first stage is text detection, where bounding boxes are drawn around the text, and the second stage is recognition, where the images are cropped to those bounding boxes and classification is performed on the letters/words. 
                
                I used NVIDIA’s TensorRT framework to speed up the model’s inference on the V100 GPU and the Jetson AGX Xavier (JAX). TensorRT approximately doubled the model’s inference throughput. The V100 GPU is one of NVIDIA’s top industry grade GPUs thus performed better, however, the JAX performed significantly well for its small form factor, so much so that it was able to perform STR in real-time with a video camera at approximately 30 fps. This fast inference real-time video application was packaged in a docker container for easy deployment.

            <br><br>
            Below are some diagrams of the project.
            The open-source code is on NVIDIA-AI-IOT GitHub:<a href="https://github.com/NVIDIA-AI-IOT/scene-text-recognition">Scene Text Recognition Github.</a>
            <br><br>
            My work was also showcased on NVIDIA's Jetson AI Labs Youtube Channel:
            </p>
            <div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;">
                    <iframe
                        src="https://www.youtube.com/embed/mFQY2XySZ0w?start=2387"
                        title="YouTube video player"
                        style="position:absolute;top:0;left:0;width:75%;height:75%;border:0;"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                        referrerpolicy="strict-origin-when-cross-origin"
                        allowfullscreen>
                    </iframe>
                </div>

            <b>Diagrams:</b><br>
            <!-- unnecessary and blurry          <img src="files/nvidia/STR_prelim_profiling.PNG" alt="NVIDIA project" style="width:100%"> -->
            <img src="files/nvidia/STR_2stage.PNG" alt="2 stage STR" style="width: 40%">
            <img src="files/nvidia/STR_end_vs_2stage.PNG" alt="NVIDIA project" style="width:40%">
            <img src="files/nvidia/STR_origEasyOCR.PNG" alt="NVIDIA project" style="width:40%">
            <img src="files/nvidia/STR_modified_EasyOCR.PNG" alt="NVIDIA project" style="width:40%">
            <img src="files/nvidia/STR_benchmarking.PNG" alt="NVIDIA project" style="width:40%">
            <img src="files/nvidia/STR_example1.PNG" alt="NVIDIA project" style="width:40%">
            <img src="files/nvidia/STR_exampl2_airport.PNG" alt="NVIDIA project" style="width:40%">
            <img src="files/nvidia/STR_example3_UPS.PNG" alt="NVIDIA project" style="width:40%">
        </div>

        <!--  ECE 598 Project Learning Based Robotics -->
        <button class="collapsible">
            <img src="files/projects/course_projects/ece598/ll.png" alt="RL Example" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">Human in-the-loop Shared Control</div>
                <div class="project-authors">Peter Du, Aamir Hasan, Abhi Kamboj</div>
                <div class="project-venue">Learning Based Robotics Course Project 2021</div>
                <div class="project-links">
                    <a href="files/projects/course_projects/ece598/ece598sg_final_project.pdf">Paper</a>  
                    <!-- <a href="link-to-code">Code</a> | 
                    <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br> 

                Learning based approaches for control and decision making have rapidly made their way into various domains as advances in data collection, compute, and algorithmic breakthroughs have allowed machines to perform at levels comparable to (or better) than human experts. 
                Despite these advances however, there are still domains where a fully autonomous agent with no human supervision or backup faces hurdles from both a regulatory and public trust perspective. 
                In particular, safety critical applications of autonomy without human oversight remains a contested approach. 
                The most prominent example of this can be seen in the autonomous driving domain where companies continue to train and require ``safety drivers'' to monitor and be ready to take over vehicle control at any given moment. 
                Even in fully autonomous products such as Waymo One's autonomous taxi service in Phoenix, safety drivers are often used during inclement weather or dispatched to take control when the vehicle senses difficulty. 
                
                The goal of our project is to look at different ways of characterising uncertainty in an autonomous policy and use them to develop control that is shared between the human and the autonomous agent. 
                To this end, we look at how this information can be obtained from reinforcement learning (RL) based policies and imitation learning based policies. 
                In the RL policies, we first consider a Deep Q-Learning (DQN) policy without modification and use the Q-values provided by the network. 
                We then retrain the policy using Bayesian Deep Learning to more explicitly characterise uncertainty. 
                We then directly try to learn a Bayesian Neural Network policy through behaviour cloning. 
                During runtime, the policies are preempted when various uncertainty thresholds are reached and a human is asked to provide the next action. 
                We compare the performance of these shared policies among each other as well as a fully autonomous policy to see which methods of requesting human input has the best overall performance. 
                 
                <div style="display: flex; justify-content: space-between; gap: 10px;">
                    <figure style="flex: 1; text-align: center;">
                        <img src="files/projects/course_projects/ece598/ll.png" alt="RL sample" style="width:50%;">
                        <figcaption>The Lunar Lander environment</figcaption>
                    </figure>
                    <figure style="flex: 1; text-align: center;">
                        <img src="files/projects/course_projects/ece598/rewards.png" alt="results-rewards" style="width:60%;">
                        <figcaption>The rewards accrued by each model</figcaption>
                    </figure>
                </div>
            </p>
        </div>


        <!-- Senior Thesis -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2021 - Senior Year/SP21-ECE499-Thesis-Kamboj, Abhi.jpeg" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">The Optimal Audio Interface For Teleoperation On An Autonomous Farm </div>
                <div class="project-authors">Abhi Kamboj</div>
                <div class="project-venue">Undergradaute Senior Thesis 2021</div>
                <div class="project-links">
                    <a href="https://www.ideals.illinois.edu/items/132047">Paper</a> 
                    <!-- <a href="link-to-code">Code</a> | 
                    <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br> 
                As robots become more prevalent, designing an efficient communication system for human-robot interaction becomes an important yet challenging problem. Visual and tactile interfaces are very common in autonomous robots and intelligent systems; however, audio-based interfaces are a relatively new and developing area. We study the scenario in which a fleet of agricultural robots need to communicate a failure case for a human operator to diagnose and respond to in a teleoperation setting. These robots must have a simple yet effective communication system so farmers that may not have robotic experience can operate them. In this thesis project, we develop an agbot simulation platform and various audio communication techniques and characterize the most effective and natural interface. First, autonomous farms of varying complexity are created using the OpenAI Gridworld simulation. Then, a user study with 11 participants is conducted with this simulation to test three audio communication methods: sounds, single-word commands, and full natural language communication. As the robots on the farm experience and report errors, the human is tasked with diagnosing them and keeping the robots going. Afterwards, the user completes a survey to determine the overall effectiveness of the system. The results suggest that the human’s perception of the system is mainly impacted by the audio communication technique not the complexity, and the single word commands provide the best interface. However, not all the results were statistically significant, potentially because of the small sample size, and further studies should be conducted on this topic to confirm the results.
                 
                <!-- <figure>
                    <img src="Abhi Research Portfolio/2021 - Senior Year/" alt="Global Tracking Transformer Architecture" style="width:30%">
                    <figcaption>Sample caption goes here</figcaption>
                </figure> -->
            </p>
        </div>
    
        <!-- ECE 484 Safe Autonomy Project-->
        <button class="collapsible">
            <img src="files/projects/course_projects/ece484/successful_left.png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">Lane Following Autonomous Vehicle Pipeline</div>
                <div class="project-authors">Neeloy Chakraborty, Abhi Kamboj, Pranav Sriram, Pavitra Shadvani</div>
                <div class="project-venue">Safe Autonomy Course Project 2021</div>
                <div class="project-links">
                    <a href="link-to-paper">Paper</a> 
                    <!-- <a href="link-to-code">Code</a> |  -->
                    <!-- <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br> 
                Lane following is a crucial component of self-driving cars, contributing to both the safety and the comfort of the experience. However, creating a robust lane-following system that can operate in unfamiliar environments is a difficult problem. Existing methods primarily use gradient and light thresholding for lane detection and advanced global positioning and localization techniques for lane following. 

                We propose a pipeline with a similar lane detection module that implements light and gradient thresholding techniques and a lane following module that dynamically generates the desired way-points for the vehicle to follow. A PD controller is then used to generate the low-level action based on the current pose of the vehicle and the generated desired way-point. The AV pipeline was tested on a simulated GEM vehicle in a Gazebo environment.
                
                For quantitative results, we evaluate the safety and quality of the system using two metrics: average distance travelled and total time taken. We ran three separate trials in the simulator (with different starting coordinates) and measured these metrics to analyze the performance and efficiency of our pipeline. We also compare the path taken by a PD controller using hardcoded way-points for the simulated track and the path taken by our system to observe a mismatch in performance. Experiments run show that our system fails primarily on straight roads whenever the lanes are failed to be detected. Our image filtering method must be further tuned to better detect lanes.
                <figure>
                    <img src="files/projects/course_projects/ece484/image_filtering.png" alt="Block Diagram of lane following steps" style="width:30%">
                    <figcaption>Steps used in image filtering unit.</figcaption>
                </figure>
            </p>
        </div>
    


        <!-- EPFL ML Building Classification -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2020 - Junior Year/sample_segmentation.png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">ML Classification of Buildings Using Images from Google Street-View</div>
                <div class="project-authors">Francisco Lozano, Abhi Kamboj, Cary Chai</div>
                <!-- <div class="project-description">Using machine learning to calculate the window to facade ratio of a building for building style classificaiton. </div> -->
                <div class="project-venue">Machine Learning Research, École Polytechnique Fédérale de Lausanne, Switzerland, 2019</div>
                <div class="project-links">
                    <a href="Abhi Research Portfolio/2020 - Junior Year/ML_Project_EPFL.pdf">Paper</a> | 
                    <a href="https://github.com/akamboj2/MLBuildingClassification">Code</a>
                    <!-- <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <figure>
                <img src="Abhi Research Portfolio/2020 - Junior Year/sample_segmentation.png" alt="Semantic segmentation of a building" style="width:20%">
                <figcaption>Semantic segmentation labeling windows, images and building facade for the building classification task, using a self made lableing tool.</figcaption>
            </figure>
            <p>
                <b>Abstract:</b>
                <br>
                Abstract—There are currently a very diverse range of building
                materials and construction styles used in cities throughout the
                world. Being able to track these materials and methods in
                buildings is important as it determines the procedures for
                rehabilitation and repairs. In order to determine the make of
                a building, recent data about the buildings is needed which is
                not always readily available. Identifying each building, if not
                automatized, would require an enormous amount of manpower.
                We hope to automatize this task, providing a framework capable
                of fetching images of buildings from Google Street-View and
                classifying them.
                <br><br>
                It was completed in collaboration with a civil engineering lab at EPFL to accurately classify buildings by their window to facade ratio using images pulled from Googl Street view of Zurich
                The project involved labeling and running a semantic segmentation task for building classification through resnet CNNs.
                This was also used for the final project of CS 433 Machine learning at EPFL in Switzerland. 
                Full paper  <a href="Abhi Research Portfolio/2020 - Junior Year/ML_Project_EPFL.pdf">Classification of Buildings using Google Street-View</a>
            </p>
        </div>


        <!-- EagleCAD Solar MPPT Charge Controller Design-->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2019 - Sophmore Year/circuit.png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">EagleCAD Solar MPPT Charge Controller Design</div>
                <div class="project-authors">Abhi Kamboj, Or Dantsker, Marco Caccamo</div>
                <!-- <div class="project-description">Designed a Maximum Power Point Tracker (MPPT) solar charge controller on for an autonomous ariel vehicle.</div> -->
                <div class="project-venue">PURE Undergraduate Research Symposium, University of Illinois, Urbana-Champaign, IL, 2018</div>
                <div class="project-links">
                    <a href="Abhi Research Portfolio/2019 - Sophmore Year/Abhi's PURE 2018 MPPT Poster_revised.png">Poster</a>
                    <!-- <a href="https://github.com/akamboj2/MLBuildingClassification">Code</a> -->
                    <!-- <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <!-- <embed src="Abhi Research Portfolio/2019 - Sophmore Year/Abhi_s PURE 2018 MPPT Poster_revised.pdf" width="800px" height="2100px" /> -->
            <p>
                <b>Description:</b> <br>
                This research was conducted in the PURE program at the University of Illinois at Urbana-Champaign, and presented at the undergraduate research symposium. More information on the motivation of the project can be found <a href="Abhi Research Portfolio/2019 - Sophmore Year/MPPTs Background Research.pptx">Background Research</a>, <a href="Abhi Research Portfolio/2019 - Sophmore Year/Pure Midsemester--Abhi_s Research.pptx">Midsemester Presentation</a>.
                <figure>
                    <img src="Abhi Research Portfolio/2019 - Sophmore Year/Abhi's PURE 2018 MPPT Poster_revised.png" alt="Solar MPPT Characge Controller Poster" style="width:100%">
                    <figcaption>Reserach poster presented at the UIUC PURE undergraduate reserach symposium in Fall 2018.</figcaption>
                </figure>
            </p>
        </div>


        <!-- NSF REU Project -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2018 - Freshman Year Summer/quad_copt.gif" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">Reinforcement Learning-based Obstacle Avoidance and Path Planning</div>
                <div class="project-authors">Authors: Abhi Kamboj, Volkan Isler</div>
                <!-- <div class="project-description"> Simulated and tested atonoumous robot navigation </div> -->
                <div class="project-venue">NSF REU, University of Minnesota, MN, 2018</div>
                <div class="project-links">
                    <a href="https://docs.google.com/document/d/1SEEsvkGU-z6koim2bKHBSuxD_w_aHCOO0D9ARiUYE9c/edit">Report</a> | 
                    <a href="https://github.com/akamboj2/AutonomousNav-RL_Research2018">Code</a> |
                    <a href="https://sites.google.com/umn.edu/rsn/projects/reu-project-abhi-kamboj">Website</a>
                </div>
            </div>
        </button>
        <div class="collapse_content">
            <p>
                <b>Description:</b>
                <br>
                In this project, I explore various navigation and obstacle avoidance techniques for autonomous robots. The motivation is to determine an efficient and effective means for robot navigation in an agricultural setting. This report covers the various projects I worked on at the Robotic Sensor Networks Laboratory under Dr. Volkan Isler. This research work was supported by the National Science Foundation’s Research Experience for Undergraduates (NSF REU) program.

                <figure>
                    <h4>VREP Obstacle Avoidance</h4>
                    <img src="Abhi Research Portfolio/2018 - Freshman Year Summer/navigation.png" alt="" style="width:50%">
                    <img src="Abhi Research Portfolio/2018 - Freshman Year Summer/obj_avoid.png" alt="" style="width:50%">
                    <h4> Using iRobot with Lidar</h4>
                    <img src="Abhi Research Portfolio/2018 - Freshman Year Summer/irobot.png" alt="" style="width:20%">
                    <h4> Using gmapping in hallways</h4>
                    <img src="Abhi Research Portfolio/2018 - Freshman Year Summer/ros-gmapping.png" alt="" style="width:40%">
                    <h4> A* Path Planning</h4>
                    <img src="Abhi Research Portfolio/2018 - Freshman Year Summer/path_plan.png" alt="" style="width:20%">

                    <figcaption>Snippets of project, see website for full details</figcaption>
                </figure>
            </p>
        </div>
       
        


        
    </div>
    </div>
    

    <script>
        /*for collapsible*/
        var coll = document.getElementsByClassName("collapsible");
        var i;
        
        for (i = 0; i < coll.length; i++) {
          coll[i].addEventListener("click", function() {
            this.classList.toggle("active");
            var content = this.nextElementSibling;
            if (content.style.maxHeight){
              content.style.maxHeight = null;
            } else {
              content.style.maxHeight = content.scrollHeight + "px";
            } 
          });
        }
        </script>
</html>
