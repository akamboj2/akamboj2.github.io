<!DOCTYPE html> 
<html>
    <head>
        <link rel="stylesheet" href="styles.css">
        <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
    </head>
    <div class="container">
    <script>
        $.get("navigation.html", function(data){
            $("#nav-placeholder").replaceWith(data);
        });
    </script>
    <div id="nav-placeholder">
    </div>
    <div class="main_text">
        <h1 id="projects"> Projects </h1>


        <!-- Sensor Fusion Project -->
        <!-- <button class="collapsible">Multi Sensor Multi Task Fusion</button>
        <div class="collapse_content"> 
            <p>
                <b>Description:</b> 
                This project aims to develop a model capable of human behavior and motion understanding a prediction.
                Details are outlined in the following proposal: <a href="files/Abhi_Kamboj_Research_Plan_Submit.pdf">Mulit Sensor Multi Task Fusion</a>
            </p>
        </div> -->

        <!--  GOOD TEMPLATE -->
        <!-- Tag Project -->
        <!-- <button class="collapsible">
            <img src="files/todo-qm.jpg" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">TITLE</div>
                <div class="project-authors">Abhi Kamboj</div>
                <div class="project-venue">Arxiv 2024</div>
                <div class="project-links">
                    <a href="link-to-paper">Paper</a> | 
                    <a href="link-to-code">Code</a> | 
                    <a href="link-to-website">Website</a>
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br> 

                 
                <figure>
                    <img src="files/todo-qm.jpg" alt="Global Tracking Transformer Architecture" style="width:30%">
                    <figcaption>Sample caption goes here</figcaption>
                </figure>
            </p>
        </div> -->
    


        <!-- FACT fusion and transfer -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2024/FACT/T-Fact_img.png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">FACT: Fusion and Cross-Modal Transfer for Zero-Shot Human Action Recognition </div>
                <div class="project-authors">Abhi Kamboj, Duy Anh Nguyen, Minh Do</div>
                <div class="project-venue">Arxiv 2024, Under review at Conference</div>
                <div class="project-links">
                    <!-- <a href="link-to-paper">Paper</a>| 
                    <a href="link-to-code">Code</a>| 
                    <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br>
                Despite living in a multi-sensory world, most AI models are limited to textual and visual interpretations of human motion and behavior. 
                Inertial measurement units (IMUs) provide a salient signal to understand human motion; however, they are challenging to use due to their uninterpretability and scarcity of their data. 
                We investigate a method to transfer knowledge between visual and inertial modalities using the structure of an informative joint representation space designed for human action recognition (HAR). 
                We apply the resulting Fusion and Cross-modal Transfer (FACT) method to a novel setup, where the model does not have access to labeled IMU data during training and is able to perform HAR with only IMU data during testing.
                Extensive experiments on a wide range of RGB-IMU datasets demonstrate that FACT significantly outperforms existing methods in zero-shot cross-modal transfer.
                
                <figure>
                    <img src="Abhi Research Portfolio/2024/FACT/FACT overview.png" alt="FACT architecture overview" style="width:40%">
                    <img src="Abhi Research Portfolio/2024/FACT/T-Fact_img.png" alt="Time continuous extension of FACT (T-FACT) Diagram" style="width:40%">
                    <figcaption>Left: An overview of the FACT method. Right: A diagram of the time continous FACT model T-FACT.</figcaption>
                </figure>
            </p>
        </div>



        <!-- Cross-modal transfer lit review -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2024/cross-modal_review/HAR_metrics (1).png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">A Survey of IMU Based Cross-Modal Transfer Learning in Human Activity Recognition
                </div>
                <div class="project-authors">Abhi Kamboj, Minh Do</div>
                <div class="project-venue">Arxiv 2024</div>
                <div class="project-links">
                    <a href="https://arxiv.org/abs/2403.15444">Paper</a> 
                    <!--  <a href="link-to-code">Code</a>| 
                    <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br>
                Despite living in a multi-sensory world, most AI models are limited to textual and visual understanding of human motion and behavior. 
                Inertial measurement sensors provide a signal for AI to understand motion, however, in practice they has been understudied due to numerous difficulties and the uniterpretability of the data to humans. 
                In fact, full situational awareness of human motion could best be understood through a combination of sensors.
                In this survey we investigate how knowledge can be transferred and utilized amongst modalities for Human Activity/Action Recognition (HAR), i.e. cross-modality transfer learning.
                We motivate the importance and potential of IMU data and its applicability in cross-modality learning as well as the importance of studying the HAR problem. 
                We categorize HAR related tasks by time and abstractness and then compare various types of multimodal HAR datasets.
                We also distinguish and expound on many related but inconsistently used terms in the literature, such as transfer learning, domain adaptation, representation learning, sensor fusion, and multimodal learning, and describe how cross-modal learning fits with all these concepts.
                We then review the literature in IMU-based cross-modal transfer for HAR.
                The two main approaches for cross-modal transfer are instance-based transfer, where instances of one modality are mapped to another (e.g. knowledge is transferred in the input space), or feature-based transfer, where the model relates the modalities in an intermediate latent space (e.g. knowledge is transferred in the feature space).
                Finally, we discuss future research directions and applications in cross-modal HAR.

                <div class="figure-container">
                    <figure>
                        <img src="Abhi Research Portfolio/2024/cross-modal_review/HAR_metrics (1).png" alt="Global Tracking Transformer Architecture">
                        <figcaption> A graph-like visualization plotting HAR-related tasks to compare each task with the other based on temporal length involved in the task as well as the abstractness or coarseness of the task.</figcaption>
                    </figure>
                    <figure>
                        <img src="Abhi Research Portfolio/2024/cross-modal_review/venn_diagram.png" alt="Global Tracking Transformer Architecture">
                        <figcaption>A comparison of sensor fusion and cross modal transfer.</figcaption>
                    </figure>
                    <figure>
                        <img src="Abhi Research Portfolio/2024/cross-modal_review/cross_modal_learning (1).png" alt="Global Tracking Transformer Architecture">
                        <figcaption>A diagram showing two types of cross-modal transfer.</figcaption>
                    </figure>

                </div>
            </p>
        </div>


        
        <!-- Netradyne DriverGPT Project -->
        <button class="collapsible">
            <img src="files/netradyne/netradyne_dash_cam.jpeg" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">DriverGPT: Training a Generative Pretrained Transformer on Time-continous Physical Data</div>
                <div class="project-authors">Abhi Kamboj, Ananya Gupta, Michael Laelli</div>
                <div class="project-venue">Netradyne Artificial Intellgience Internship, San Diego, CA, 2022</div>
                <div class="project-links">
                    <!-- <a href="link-to-paper">Paper</a>| 
                    <a href="link-to-code">Code</a>| 
                    <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Description:</b> <br>
                Generative Pretrained Transformers (GPT), e.g., ChatGPT, have recently changed the landscape of AI research with their size, scale, and performance. Researchers have discovered that with ample data and compute resources, this architecture performs very well and shows "emergent behavior," i.e., the ability to perform well or demonstrate knowledge on tasks it was not explicitly trained on. Netradyne is a fleet safety company that focuses on dash camera driving analytics, and they have vast amounts of driver data, providing a unique setting to leverage GPT-style models. The DriverGPT project aims to learn to predict future driver behaviors based on previous time steps. This could be used to predict short-term trajectories, such as potential collisions, or long-term behaviors, such as the overall trends of a driver's safety score and how it compares to other drivers. Nonetheless, extending GPT to driving data presented many challenges, such as how to tokenize continuous data or how to use GPT on time series data from devices with different frame rates and specifications. Many small-scale experiments were conducted to address these questions.
            <!-- 
                <figure>
                    <img src="files/netradyne/GTR.png" alt="Global Tracking Transformer Architecture" style="width:30%">
                    <figcaption>Transformer architectures for multi object tracking (ours is called Trackformer++)</figcaption>
                </figure> -->
            </p>
        </div>

        <!-- Robotics Pretraining Project -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2023/robot_graping_figs/affordance-img-thumbnail.png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">A Brief Survey on Leveraging Large Scale Vision Models for Enhanced Robot Grasping</div>
                <div class="project-authors">Abhi Kamboj, Katherine Driggs-Campbell</div>
                <!-- <div class="project-description">The goal of this internship project was to create a novel multi object tracking architecture using transformers.</div> -->
                <div class="project-venue">Arxiv 2023</div>
                <div class="project-links">
                    <a href="https://arxiv.org/abs/2406.11786">Paper</a>
                    <!-- <a href="link-to-code">Code</a>| 
                    <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br>
                Robotic grasping presents a difficult motor task in real-world scenarios, constituting a major hurdle to the deployment of capable robots across various industries. 
                Notably, the scarcity of data makes grasping particularly challenging for learned models. 
                Recent advancements in computer vision have witnessed a growth of successful unsupervised training mechanisms predicated on massive amounts of data sourced from the Internet, and now nearly all prominent models leverage pretrained backbone networks. 
                Against this backdrop, we begin to investigate the potential benefits of large-scale visual pretraining in enhancing robot grasping performance. 
                This preliminary literature review sheds light on critical challenges and delineates prospective directions for future research in visual pretraining for robotic manipulation.
                
                <figure>
                    <img src="Abhi Research Portfolio/2023/robot_graping_figs/affordance-img.png" alt="Affordance prediction used for predicting robot grasps" style="width:30%">
                    <figcaption>This illustrates how visual pretraining can be used to teach affordance prediction models that guide a robot manipulator to the best picking position. This image is from <a /href="https://arxiv.org/pdf/2107.00646">"Learning to See before Learning to Act: Visual Pre-training for Manipulation
""                    </a></figcaption>
                </figure>
            </p>
        </div>
        

        <!-- MOT Transformers Survey -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2023/transformers_review_figs/TransTrack.png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">The Progression of Transformers from Language to Vision to MOT: A Literature Review on Multi-Object Tracking with Transformers</div>
                <div class="project-authors">Abhi Kamboj</div>
                <div class="project-venue">Arxiv 2023</div>
                <div class="project-links">
                    <a href="https://arxiv.org/abs/2406.16784">Paper</a> 
                    <!-- <a href="link-to-code">Code</a>| 
                    <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Abstract:</b> <br>
                The transformer neural network architecture allows for autoregressive sequence-to-sequence modeling through the use of attention layers. It was originally created with the application of machine translation but has revolutionized natural language processing. Recently, transformers have also been applied across a wide variety of pattern recognition tasks, particularly in computer vision. In this literature review, we describe major advances in computer vision utilizing transformers. We then focus specifically on Multi-Object Tracking (MOT) and discuss how transformers are increasingly becoming competitive in state-of-the-art MOT works, yet still lag behind traditional deep learning methods.
                
                <figure>
                    <img src="Abhi Research Portfolio/2023/transformers_review_figs/Transformer.png" alt="Global Tracking Transformer Architecture" style="width:30%">
                    <img src="Abhi Research Portfolio/2023/transformers_review_figs/TransTrack.png" alt="Global Tracking Transformer Architecture" style="width:30%">
                    <figcaption>The left shows the original <a href="https://arxiv.org/abs/1706.03762"> Transformer </a> architectecture. The right shows an extension to the architurecture of object tracking, implemented as <a href="https://arxiv.org/abs/2012.15460">TransTrack</a> </figcaption>
                </figure> 
            </p>
        </div>



        <!-- Netradyne MOT Project -->
        <button class="collapsible">
            <img src="files/netradyne/Trackformer++.png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">Multi Object Tracking with Transformers</div>
                <div class="project-authors">Abhi Kamboj, Michael Laelli</div>
                <!-- <div class="project-description">The goal of this internship project was to create a novel multi object tracking architecture using transformers.</div> -->
                <div class="project-venue">Netradyne Machine Learning Internship, San Diego, CA, 2022</div>
                <div class="project-links">
                    <!-- <a href="link-to-paper">Paper</a>| 
                    <a href="link-to-code">Code</a>| 
                    <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Description:</b> <br>
                The goal of this internship project was to create a novel multi object tracking architecture using transformers. 
                This is a reserach project investigating various forms of speech/audio communication for managing a fleet of autonomous robots.
                I investigated existing Multi Object trakcing methods such as Global Tracking Transformers (GTR), Trackformer and MOTR and sought to improve them.
                All the existing methods do not utilize a transformers capability to deal with long range dependencies.
                GTR feeds in detections to a transformer to perform tracking over a window of frames, however, they lose contextual information but only feeding in detections, and they rely heavily on an accurate detector.
                On the other hand, Trackformer and MOTR autogressively feed in pairs of frames, not just detections, to detect and track objects in the next frame, however, they only work with a window of 2 frames at once.
                During this internship I helped develop an object tracking architecture that fuses multiple frames of features into the transformer and outputs multiple frames of object detections and tracks as output.
                
                <br><br>
                Work on this project is ongoing and experiments are being conducted. We refer to our model as <b>Trackformer++</b> as it builds off of Trackformer. A visual comparison is shown below.
                <figure>
                    <img src="files/netradyne/GTR.png" alt="Global Tracking Transformer Architecture" style="width:30%">
                    <img src="files/netradyne/Trackformer.png" alt="Trackformer Architecture" style="width:30%">
                    <img src="files/netradyne/Trackformer++.png" alt="Trackformer++ (Our) Architecture" style="width:25%">
                    <figcaption>Transformer architectures for multi object tracking (ours is called Trackformer++)</figcaption>
                </figure>
            </p>
        </div>


        <!-- Agbot Project -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2021 - Senior Year/auto_farm30x30.gif" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">Examining Audio Communication Mechanisms for Supervising Fleets of Agricultural Robots</div>
                <div class="project-authors">Abhi Kamboj, Tianchen Ji, Katherine Driggs-Campbell</div>
                <!-- <div class="project-description">This is a research project investigating various forms of speech/audio communication for managing a fleet of autonomous robots.</div> -->
                <div class="project-venue">Oral in 2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)
                </div>
                <div class="project-links">
                    <a href="link-to-paper">Paper</a> | 
                    <a href="link-to-code">Code</a> | 
                    <a href="link-to-website">Website</a>
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <figure>
                    <img src="Abhi Research Portfolio/2021 - Senior Year/auto_farm30x30.gif" alt="Semantic segmentation of a building" style="width:20%">
                    <img src="Abhi Research Portfolio/2021 - Senior Year/small_grid_errors.png" alt="Semantic segmentation of a building" style="width:30%">
                    <img src="Abhi Research Portfolio/2021 - Senior Year/terrasentia.jpg" alt="Semantic segmentation of a building" style="width:30%">
                    <figcaption>Example of the simulation running, user interface, and real robots they represent.</figcaption>
                </figure>

                <b>Abstract:</b>
                <br>
                Agriculture is facing a labor crisis, leading to increased interest in fleets of small, under-canopy robots (agbots) that can perform precise, targeted actions (e.g., crop scouting, weeding, fertilization), while being supervised by human operators remotely. 
                However, farmers are not necessarily experts in robotics technology and will not adopt technologies that add to their workload or do not provide an immediate payoff.
                In this work, we explore methods for communication between a remote human operator and multiple agbots and examine the impact of audio communication on the operator's preferences and productivity.
                We develop a simulation platform where agbots are deployed across a field, randomly encounter failures, and call for help from the operator. 
                As the agbots report errors, various audio communication mechanisms are tested to convey which robot failed and what type of failure occurs. 
                The human is tasked with verbally diagnosing the failure while completing a secondary task. 
                A user study was conducted to test three audio communication methods: earcons, single-phrase commands, and full sentence communication. 
                Each user completed a survey to determine each method's overall effectiveness and preferences. 
                Our results suggest that the system using short phrases is the most positively perceived by participants and may allow for the human to complete the secondary task more efficiently. 
                <br><br>
                <b>Full paper accepted as Oral Presentation @ IEEE ROMAN 2022, Naples Italy: <a href="Abhi Research Portfolio/2022 - Grad School/ROMAN_2022_Audio_Interaction (1).pdf">Examining Audio Communication Mechanisms for Supervising Fleets of Agricultural Robots</a></b>
            </p>
        </div>

        <!-- NVIDIA STR PROJECT -->
        <button class="collapsible">
            <img src="files/nvidia/STR_exampl2_airport.PNG" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">Developing Scene Text Recognition on Jetson Devices using TensorRT</div>
                <div class="project-authors">Abhi Kamboj</div>
                <!-- <div class="project-description">Using TensorRT to develop scene text recogntion models on the Jetson AGX Xavier and benchmarking SOTA models. </div> -->
                <div class="project-venue">NVIDIA Embedded AI Internship, San Jose, CA, 2021</div>
                <div class="project-links">
                    <!-- <a href="link-to-paper">Paper</a>|  -->
                    <a href="https://github.com/NVIDIA-AI-IOT/scene-text-recognition">Code</a>
                    <!-- <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <p>
                <b>Description:</b><br>
                I worked in NVIDIA’s Autonomous machines unit, under the Jetson Dev Tech team. 
                The Jetson product line is NVIDIA’s embedded AI GPU for edge IoT and mobile robotics applications. 
                The goal of my project was to develop an open-source Scene Text Recognition (STR) system for NVIDIA partners to use, as well as for NVIDIA’s own internal projects to use. 
                I initially researched and benchmarked various state of the art STR models (CSTR, STRN, EasyOCR) and chose the 2 stage EasyOCR framework to further develop, as it performed the best. 
                The first stage is text detection, where bounding boxes are drawn around the text, and the second stage is recognition, where the images are cropped to those bounding boxes and classification is performed on the letters/words. 
                
                I used NVIDIA’s TensorRT framework to speed up the model’s inference on the V100 GPU and the Jetson AGX Xavier (JAX). TensorRT approximately doubled the model’s inference throughput. The V100 GPU is one of NVIDIA’s top industry grade GPUs thus performed better, however, the JAX performed significantly well for its small form factor, so much so that it was able to perform STR in real-time with a video camera at approximately 30 fps. This fast inference real-time video application was packaged in a docker container for easy deployment.

            Below are some diagrams of the project.
            The open-source code is on NVIDIA-AI-IOT GitHub:<a href="https://github.com/NVIDIA-AI-IOT/scene-text-recognition">Scene Text Recognition Github</a>
            </p>
            <!-- unnecessary and blurry          <img src="files/nvidia/STR_prelim_profiling.PNG" alt="NVIDIA project" style="width:100%"> -->
            <img src="files/nvidia/STR_2stage.PNG" alt="2 stage STR" style="width: 40%">
            <img src="files/nvidia/STR_end_vs_2stage.PNG" alt="NVIDIA project" style="width:40%">
            <img src="files/nvidia/STR_origEasyOCR.PNG" alt="NVIDIA project" style="width:40%">
            <img src="files/nvidia/STR_modified_EasyOCR.PNG" alt="NVIDIA project" style="width:40%">
            <img src="files/nvidia/STR_benchmarking.PNG" alt="NVIDIA project" style="width:40%">
            <img src="files/nvidia/STR_example1.PNG" alt="NVIDIA project" style="width:40%">
            <img src="files/nvidia/STR_exampl2_airport.PNG" alt="NVIDIA project" style="width:40%">
            <img src="files/nvidia/STR_example3_UPS.PNG" alt="NVIDIA project" style="width:40%">
        </div>



        <!-- EPFL ML Building Classification -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2020 - Junior Year/sample_segmentation.png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">ML Classification of Buildings Using Images from Google Street-View</div>
                <div class="project-authors">Francisco Lozano, Abhi Kamboj, Cary Chai</div>
                <!-- <div class="project-description">Using machine learning to calculate the window to facade ratio of a building for building style classificaiton. </div> -->
                <div class="project-venue">Machine Learning Research, École Polytechnique Fédérale de Lausanne, Switzerland, 2019</div>
                <div class="project-links">
                    <a href="Abhi Research Portfolio/2020 - Junior Year/ML_Project_EPFL.pdf">Paper</a> | 
                    <a href="https://github.com/akamboj2/MLBuildingClassification">Code</a>
                    <!-- <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <figure>
                <img src="Abhi Research Portfolio/2020 - Junior Year/sample_segmentation.png" alt="Semantic segmentation of a building" style="width:20%">
                <figcaption>Semantic segmentation labeling windows, images and building facade for the building classification task, using a self made lableing tool.</figcaption>
            </figure>
            <p>
                <b>Abstract:</b>
                <br>
                Abstract—There are currently a very diverse range of building
                materials and construction styles used in cities throughout the
                world. Being able to track these materials and methods in
                buildings is important as it determines the procedures for
                rehabilitation and repairs. In order to determine the make of
                a building, recent data about the buildings is needed which is
                not always readily available. Identifying each building, if not
                automatized, would require an enormous amount of manpower.
                We hope to automatize this task, providing a framework capable
                of fetching images of buildings from Google Street-View and
                classifying them.
                <br><br>
                It was completed in collaboration with a civil engineering lab at EPFL to accurately classify buildings by their window to facade ratio using images pulled from Googl Street view of Zurich
                The project involved labeling and running a semantic segmentation task for building classification through resnet CNNs.
                This was also used for the final project of CS 433 Machine learning at EPFL in Switzerland. 
                Full paper  <a href="Abhi Research Portfolio/2020 - Junior Year/ML_Project_EPFL.pdf">Classification of Buildings using Google Street-View</a>
            </p>
        </div>



        <!-- EagleCAD Solar MPPT Charge Controller Design-->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2019 - Sophmore Year/circuit.png" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">EagleCAD Solar MPPT Charge Controller Design</div>
                <div class="project-authors">Authors: Abhi Kamboj, Or Dantsker, Marco Caccamo</div>
                <!-- <div class="project-description">Designed a Maximum Power Point Tracker (MPPT) solar charge controller on for an autonomous ariel vehicle.</div> -->
                <div class="project-venue">PURE Undergraduate Research Symposium, University of Illinois, Urbana-Champaign, IL, 2018</div>
                <div class="project-links">
                    <!-- <a href="Abhi Research Portfolio/2020 - Junior Year/ML_Project_EPFL.pdf">Paper</a>| 
                    <a href="https://github.com/akamboj2/MLBuildingClassification">Code</a> -->
                    <!-- <a href="link-to-website">Website</a> -->
                </div>
            </div>
        </button>
        <div class="collapse_content"> 
            <!-- <embed src="Abhi Research Portfolio/2019 - Sophmore Year/Abhi_s PURE 2018 MPPT Poster_revised.pdf" width="800px" height="2100px" /> -->
            <p>
                <b>Description:</b> <br>
                This research was conducted in the PURE program at the University of Illinois at Urbana-Champaign, and presented at the undergraduate research symposium. More information on the motivation of the project can be found <a href="Abhi Research Portfolio/2019 - Sophmore Year/MPPTs Background Research.pptx">Background Research</a>, <a href="Abhi Research Portfolio/2019 - Sophmore Year/Pure Midsemester--Abhi_s Research.pptx">Midsemester Presentation</a>.
                <figure>
                    <img src="Abhi Research Portfolio/2019 - Sophmore Year/Abhi's PURE 2018 MPPT Poster_revised.png" alt="Solar MPPT Characge Controller Poster" style="width:100%">
                    <figcaption>Reserach poster presented at the UIUC PURE undergraduate reserach symposium in Fall 2018.</figcaption>
                </figure>
            </p>
        </div>

        <!-- NSF REU Project -->
        <button class="collapsible">
            <img src="Abhi Research Portfolio/2018 - Freshman Year Summer/quad_copt.gif" alt="Project Image" class="collapsible-image">
            <div class="collapsible-content">
                <div class="project-title">Reinforcement Learning-based Obstacle Avoidance and Path Planning</div>
                <div class="project-authors">Authors: Abhi Kamboj, Volkan Isler</div>
                <!-- <div class="project-description"> Simulated and tested atonoumous robot navigation </div> -->
                <div class="project-venue">NSF REU, University of Minnesota, MN, 2018</div>
                <div class="project-links">
                    <a href="https://docs.google.com/document/d/1SEEsvkGU-z6koim2bKHBSuxD_w_aHCOO0D9ARiUYE9c/edit">Report</a> | 
                    <a href="https://github.com/akamboj2/AutonomousNav-RL_Research2018">Code</a> |
                    <a href="https://sites.google.com/umn.edu/rsn/projects/reu-project-abhi-kamboj">Website</a>
                </div>
            </div>
        </button>
        <div class="collapse_content">
            <p>
                <b>Description:</b>
                <br>
                In this project, I explore various navigation and obstacle avoidance techniques for autonomous robots. The motivation is to determine an efficient and effective means for robot navigation in an agricultural setting. This report covers the various projects I worked on at the Robotic Sensor Networks Laboratory under Dr. Volkan Isler. This research work was supported by the National Science Foundation’s Research Experience for Undergraduates (NSF REU) program.

                <figure>
                    <h4>VREP Obstacle Avoidance</h4>
                    <img src="Abhi Research Portfolio/2018 - Freshman Year Summer/navigation.png" alt="" style="width:50%">
                    <img src="Abhi Research Portfolio/2018 - Freshman Year Summer/obj_avoid.png" alt="" style="width:50%">
                    <h4> Using iRobot with Lidar</h4>
                    <img src="Abhi Research Portfolio/2018 - Freshman Year Summer/irobot.png" alt="" style="width:20%">
                    <h4> Using gmapping in hallways</h4>
                    <img src="Abhi Research Portfolio/2018 - Freshman Year Summer/ros-gmapping.png" alt="" style="width:40%">
                    <h4> A* Path Planning</h4>
                    <img src="Abhi Research Portfolio/2018 - Freshman Year Summer/path_plan.png" alt="" style="width:20%">

                    <figcaption>Snippets of project, see website for full details</figcaption>
                </figure>
            </p>
        </div>
       
    </div>
    </div>
    

    <script>
        /*for collapsible*/
        var coll = document.getElementsByClassName("collapsible");
        var i;
        
        for (i = 0; i < coll.length; i++) {
          coll[i].addEventListener("click", function() {
            this.classList.toggle("active");
            var content = this.nextElementSibling;
            if (content.style.maxHeight){
              content.style.maxHeight = null;
            } else {
              content.style.maxHeight = content.scrollHeight + "px";
            } 
          });
        }
        </script>
</html>
